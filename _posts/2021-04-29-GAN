# Generative Adversarial Networks(NIPS 2014)

![gan1](https://user-images.githubusercontent.com/33116818/116511814-faa34e80-a901-11eb-8e35-966cc9aaa166.PNG)

위의 그림 중 노란색 부분이 실제 이미지에 해당하는 부분이고, 그 외의 부분은 GAN을 통해 만들어진 이미지입니다.



# 확률분포

이를 위해 선수 지식인 확률 분포에 대해서 설명합니다.



확률분포는 확률변수가 특정한 값을 가질 확률을 나타내는 함수를 의미합니다.



예를 들어 주사위를 던졌을 때 나올 수 있는 확률변수 X라고 하면



 ##\- 확률변수 X는 1,2,3,4,5,6의 값을 가질 수 있습니다.



 ##- P(X=1)는 1/6의 확률 값을 가지게 됩니다.



 \##- P(X=1) = P(X=2) = P(X=3) = P(X=4) = P(X=5) = P(X=6) = 1/6의 확률 값을 갖게 됩니다.







*이러한 확률 분포는 크게 2가지 유형을 가지고 있습니다.*



1. 이산확률분포



​		확률변수 X의 개수를 정확히 셀 수 있을 때 이산확률분포라 말합니다.



2. 연속확률분포



​		확률변수 X의 개수를 정확히 셀 수 없을 때 연속활률분포라 말합니다. (확률밀도 함수를 이용해 분포를 표현)



​		연속적인 값의 예시로는 키, 달리기 성적 등을 말할 수 있습니다.





# 이미지 데이터에 대한 확률분포

이미지 데이터는 다차원 특징 공간의 한 점으로 표현됩니다.



​	\- 이미지의 분포를 근사하는 모델을 학습할 수 있습니다.

 

사람의 얼굴에는 통계적인 평균치 즉, 눈썹의 길이, 코의 길이 등이 존재하고 모델은 이를 수치적으로 표현할 수 있습니다.



이미지 데이터에 대한 확률 분포란 이미지에서 다양한 특징들이 각각의 확률 변수가 되는 분포를 의미합니다. 



아래의 그림은 특징공간 상에서의 확률분포를 나타냅니다. (다변수 확률분포의 예시)



Hidden Layer에서 Dimension을 2라고 설정하면 두개의 특징에 대한 값을 얻게 되는데 만약 예를 들어 Dimension 1은 코의 길이 Dimension 2는 눈의 모양이라고 하면 인간의 얼굴에 대해 각각의 특징값에 대한 확률 분포를 표현한다면 아래와 같은 모양을 띄게 된다.



![image](https://user-images.githubusercontent.com/33116818/116512995-e1030680-a903-11eb-8f70-74e069bd9d07.png)





생성 모델 GAN은 실존하지 않지만 있을 법한 이미지를 생성할 수 있는 모델을 의미합니다.



![image](https://user-images.githubusercontent.com/33116818/116513043-f37d4000-a903-11eb-9d2a-0a99b9560420.png)



Discriminative는 특정한 디시전바운더리를 학습하는 형태입니다.



Generative는 각각의 클래스에 대해서 적절한 분포를 학습하는 형태입니다.





확률분포를 잘 학습할 수 있다면 모델은 통계적인 평균치를 내재할 수 있습니다.



확률분포가 인간의 얼굴에 대한 분포를 잘 학습했다고 하면 확률 값이 높은 부분에 대한 변수 즉, 이미지를 샘플링을 하게 되면 A와 같은 형태의 이미지를 나오게 됩니다.



반면 확률 값이 낮은 부분에 대한 데이터를 샘플링하게 되면 B와 같은 형태의 이미지가 나오게  됩니다.



![image](https://user-images.githubusercontent.com/33116818/116513231-37704500-a904-11eb-9079-2fcdc9cad832.png)





# 생성 모델의 목표

이미지 데이터의 분포를 근사하는 모델 G를 만드는 것이 생성 모델의 목표입니다.



모델 G가 잘 작동한다는 의미는 원래 이미지들의 분포를 잘 모델링할 수 있다는 것을 의미합니다.



GAN을 이용해 학습되는 과정은 아래와 같습니다.



![gan3](https://user-images.githubusercontent.com/33116818/116514484-0e50b400-a906-11eb-9723-4e84583ad2aa.PNG)



모델 G는 원래 데이터(이미지)의 분포를 근사할 수 있도록 학습되고 모델 G의 학습이 잘 되었다면 원본 데이터의 분포를 근사할 수 있습니다.

학습이 잘 되었다면 통계적으로 평균적인 특징을 가지는 데이터를 쉽게 생성할 수 있습니다.



![image](https://user-images.githubusercontent.com/33116818/116514522-1dcffd00-a906-11eb-846d-01e0b7d96488.png)





# Generative Adversarial Networks (GAN)

생성자(Generator)와 판별자(Discriminator) 두개의 네트워크를 활용한 생성 모델입니다.



아래의 목적함수를 통해 생성자는 이미지 분포를 학습할 수 있습니다.



![image](https://user-images.githubusercontent.com/33116818/116514562-2b858280-a906-11eb-9627-3e9c4d4b3bbf.png)





함수 V는 D와 G의 두개의 함수로 구성됩니다.



V라는 함수의 값을 G는 낮추고자 하고 D는 높이고자 합니다.



![image](https://user-images.githubusercontent.com/33116818/116514604-39d39e80-a906-11eb-8958-aaefb1f403d9.png)



Pdata는 원본데이터의 분포를 의미합니다.



원본데이터에서 한 개의 데이터 x를 샘플링해서 D에 넣고 로그를 취한 후 그것에 대한 기대값(평균값)을 의미합니다.



![image](https://user-images.githubusercontent.com/33116818/116514613-3c35f880-a906-11eb-9662-eb8f9c537749.png)



생성자는 항상 노이즈 벡터로부터 입력을 받아 새로운 이미지를 만들 수 있습니다.



Pz는 하나의 노이즈를 샘플링할 수 있는 분포를 의미합니다.



그러한 분포에서 랜덤하게 하나의 노이즈를 샘플링한 후 생성자 G에 넣고 가짜이미지를 생성한 후 D넣고 -를 붙이고 1을 더한 후 로그를 취한 후의 평균값을 의미합니다.



G는 노이즈 벡터인 z를 받아 새로운 이미지 인스턴스를 만들 수 있습니다.



D는 이미지 x를 받아 이미지가 얼마나 진짜 같은지에 대한 확률값을 출력으로 내보내게 됩니다.



진짜 이미지에는 1을 부여 가짜 이미지에는 0을 부여하고 출력값은 확률값으로 0부터 1사이에 값으로 존재하게 됩니다.





# GAN에서의 기대값 계산 방법

프로그램상에서 기댓값을 계산하는 가장 간단한 방법은 단순히 모든 데이터를 하나씩 확인하여 식에 대입한 뒤에 평균을 계산하면 됩니다.



기대값은 여러 개의 값을 평균낼 때 사용합니다.



![image](https://user-images.githubusercontent.com/33116818/116514798-77d0c280-a906-11eb-95fd-4da260b80e2a.png)



기대값 공식은 기대값은 모든 사건에 대해 확률을 곱하면서 더하여 계산할 수 있습니다.



![image](https://user-images.githubusercontent.com/33116818/116514821-81f2c100-a906-11eb-81a9-7b93d9d80d71.png)





# GAN의 수렴과정

공식의 목표는 Pg(생성자의 분포)가 Pdata(원본 학습데이터의 분포)를 잘 따를 수 있도록 만드는 것입니다.



D(G(z))는 학습이 다 이루어진 다음 가짜 이미지와 진짜 이미지를 구분할 수 없기 때문에 50퍼센트의 값을 내보내게 됩니다.



![image](https://user-images.githubusercontent.com/33116818/116514875-93d46400-a906-11eb-8e79-4b21a0d9f2d6.png)



Z 공간에서 z를 샘플링해서 생성자에 넣고 이러한 과정을 z도메인에서 x의 도메인으로 맵핑이 되되는 것을 표현할 수 있습니다.
