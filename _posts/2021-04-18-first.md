---
layout: single
title:  "Batch Normalization."
---

# Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift
Sergey loffe, Christian Szegedy (ICML 2015)

## Normalization 
Normalization은 데이터의 범위를 조정하는 방법으로 이를 통해 값 자체를 0과 1사이의 값으로 조정할 수 있다.
X’ = X-X(min) / X(max)-X(min) 이와 같은 식을 통해 범위를 조정하게 된다.

![6](https://user-images.githubusercontent.com/33116818/116046927-815fed80-a6ae-11eb-81a4-934b7fbfa0cc.PNG)

## Standardization
Standardization은 데이터의 범위를 조정하는 방법으로 이를 통해 평균이 0, 분산이 1이 되도록 조정할 수 있다.
X’ = X-μ / σ 이와 같은 식을 통해 범위를 조정하게 된다.

![2](https://user-images.githubusercontent.com/33116818/116047014-99d00800-a6ae-11eb-9045-d0ecc21afbf0.PNG)

## Dataset shift
예를 들어 성별 간 구분되는 특징 A를 분류하는 Task가 있다고 한다.
Train Data = 성별 간 특징을 나타내는 데이터 + 청년층으로 이루어져 있고, Test Data = 성별 간 특징을 나타내는 데이터 + 노년층으로 이루어져 있다.

![1](https://user-images.githubusercontent.com/33116818/116047040-9fc5e900-a6ae-11eb-9883-a9d8c2095302.PNG)

이럴 경우 Train과 Test의 데이터 분포가 달라 성능 하락의 문제를 일으키게 된다.

Dataset shift는 3가지 종류로 나눌 수 있다.
1)	독립 변수에서의 분포 변화 : Covariate Shift
    Internal Covariate Shift
    ![3](https://user-images.githubusercontent.com/33116818/116047025-9ccaf880-a6ae-11eb-827e-2654e03e7b30.PNG)
    

    각 layer를 통과하는 Input의 분포가 변화하는 현상을 맗나다. 
    본 논문에서는 activation function으로 Sigmoid를 사용한다. 
    역전파를 시행할 때 Sigmoid를 편미분하게 되는데 layer를 통과하다 Gradient Vanishing Problem이 발생하게 된다. (서로 다른 분포에 최적화하는 과정에서 Sigmoid의 양 극단으로 치우칠 확률이 증가)
    대안으로 ReLu / Careful Initialization / Small Learning rate을 말할 수 있다. 
2)	목적 변수에서의 분포 변화 : Prior Probability Shift
3)	독립 변수와 목적 변수 사이의 관계 변화 : Concept Shift

## Batch Normalization
1) Channel 별 feature를 독립적으로 Normalize

![4](https://user-images.githubusercontent.com/33116818/116047050-a0f71600-a6ae-11eb-8616-a1775dd15638.PNG)

Normalize를 하면 기존의 범위보다는 줄어들게 된다.

Sigmoid Function을 통과할 때 범위가 줄어들게 되면 위의 그림에서 빨간색 부분에만 activation Function이 적용된다. 

그렇게 되면 빨간색 부분은 Linear한 부분으로 Nonlinear함수의 효과를 제대로 받을 수 없다. 이를 위해 감마와 베타를 추가해서 보완하는 형식을 취한다.

2) Mini-batch 단위로 Normalize

전체 Batch의 통계량을 계산하는 것은 비효율적이다. 

그러므로 Mini-batch로도 어느 정도 근사 가능하다.(적당히 큰 경우)

### Batch Normalization : Algorithm
![5](https://user-images.githubusercontent.com/33116818/116047051-a2c0d980-a6ae-11eb-9a9d-484c8aa0fedd.PNG)

Batch Normalizatio의 Algorithm은 위와 같다.

BN Layer
• 신경망의 어떠한 activation에도 추가할 수 있음 (paper에서는 activation function 이전에 적용)
• 미니배치 간 완전히 독립적인 것은 아님 → learnable parameter γ와 β가 있기 때문
• γ와 β로 표현되는 선형변환의 input이 normalized → 안정적인 수렴
• 미니배치에 따라 다양한 normalization 수행 → Generalization

장점
큰 learning rate 사용 가능 : 파라미터의 작은 변화가 gradient를 급격하게 증폭시키지 않도록
[Normal Case]
Larger learning rate → Increasing the scale of layer parameters → Amplify the gradient backpropagation → Lead to the model explosion
Regularization 효과 : mini-batch에서 등장하는 다양한 샘플에 의한 일반화 성능 증가 (dropout과 같은 역할)

단점
Batch size에 의존적이다. (작을수록 성능 하락)
Recurrent 기반 모델에 적용하기 어렵다.

출처 : KoreaUniv DSBA 김탁영

https://www.youtube.com/watch?v=4jAyXi7byd8&list=LL&index=4&t=12s
